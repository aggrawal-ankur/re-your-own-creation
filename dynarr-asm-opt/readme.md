# Optimizing dynarr.asm

***Date: February 03, 2026***

Now I've a fully working handwritten assembly for dynarr.asm. The next thing I want to do is to understand the various optimization tricks that can be applied on this assembly and implement them.

I don't have to do any guess-work because I can take help from GCC itself. I'll read, analyze and understand GCC's output at -O1 and -O2 and implement those tricks in my assembly. That's my next target.

My approach is simple.

First, I don't know if there is a canonical source where every compiler optimization term is mentioned. I am not going to cry this time. Doesn't exist, great.

Second, even if there was a canonical source, or I get one generated by ChatGPT, reading that would feel boring because I don't know how the shape of assembly changes when that optimization trick is applied on baseline assembly.

My approach is that I'll generate -O1 and -O2 assembly from GCC and analyze every procedure line by line, understand how the intent is expressed here and how is it different from my handwritten assembly.

Next I'll feed my analysis and gcc's output to ChatGPT, because ChatGPT has knowledge that I don't have, plus it can access internet quickly. ChatGPT will name the **compiler optimization term** involved here.

If I were to optimize GCC's -O0 output, that'd have felt boring and non-living. I've already removed that by writing a baseline assembly myself. Now it will feel extremely personal and related, something I need when I want to make real progress.

I don't know how I will manage the findings, so I am letting findings come first. Later, the solutions will emerge too.

Thoughts here will be raw, except the technical stuff, which is either correct or incorrect, nothing in between.

## GCC Output

```bash
gcc dynarr/dynarr.c -S -masm=intel -O1 -fno-asynchronous-unwind-tables -fno-dwarf2-cfi-asm -masm=intel
```

Line stats: 483

# First Impressions (Day 1)

I've 2 immediate findings.
  1. Base pointer omission
  2. Different offset calculation method

Base pointer omission is something I'm aware of.

## Offset calculation

Since the layout of the dynamic array is simple and consistent, there is no need for calculations like: `[rdi + 8*1]`. GCC is using:
```asm
mov	QWORD PTR [r12], rax
mov	QWORD PTR 8[r12], rbx
mov	QWORD PTR 24[r12], rbp
mov	QWORD PTR 16[r12], 0
```
.... throughout the assembly.

I have used stuff like: `[rdi + 8*1]`, which is better to comprehend the layout when you are a newbie. When I was starting, I've to think like: "0-7 for ptr, 8-15 for elem_size, so [rdi + 8*1] is the right one".

When I look at GCC's version, I can immediate notice that it is better. `8[rdi]` is basically elem_size. It aligns with how I started myself to reason offset calculation in organized memory layouts. `QWORD PTR 8[rdi]` means "load 8 bytes starting from 8th byte from the base represented by rdi".

That's the first thing I am going to change.

### After math

I've made the changes and running the tests again showed that pushOne had a segfault, which means memcpy failed. init and extend worked as expected.

This line was wrong:
```asm
mov  rdi, QWORD PTR  8[r14]    # arr->ptr

# Correct one
mov  rdi, QWORD PTR   [r14]    # arr->ptr
```

That means, I wrongly updated one pattern for address calculation.

pushOne works now and pushMany is fine. getelement is the next wrong piece, a segfault. I presume the same problem, and it is.
```asm
mov  rax, QWORD PTR 8[r14]    # arr->ptr

# Correct one
mov  rdi, QWORD PTR  [r14]    # arr->ptr
```

getelement is working, setidx is working, the next segfault is at mergedyn2dyn. It should be at memcpy and I presume the same issue, and it is. All tests passed.

## Base Pointer Omission

For this kind of stuff, I am gonna use my [knowledge-base](https://github.com/aggrawal-ankur/knowledge-base), which is also available on GitHub. It has been a long time updating it.

**IMPORTANT NOTE: It already contains a directory for compiler optimizations which has tables I generated with ChatGPT when I was attempting to learn compiler optimization terms during Nov 21, 2025 to Dec 21, 2025. I will clear that stuff before I add anything.**

**IMPORTANT NOTE: I've removed 6/7 files as the last one is literally about FPO only. Although it contains mostly unnecessary stuff and I've removed that part.**

It can be found here: [compiler-optimizations/fpo.md](https://github.com/aggrawal-ankur/knowledge-base/blob/main/compiler-optimizations/fpo/fpo.md)

---

BPO/FPO implemented. 47 lines reduced, builds, runs, and passes all tests.

---

Done with the day.

# Day 2

***February 04, 2026***

Started my day with correcting an issue from yesterday. As I removed rbp, that misaligns rsp and I have to ensure the calculation still holds. It didn't hold at most of the places yet it worked. That's UB.

The first optimization trick for today is **Partial Redundancy Elimination (PRE)**. Although I think it only touches PRE and is not heavy PRE in any sense.

***We make failure the default return and hoist that branch early, which can be overwritten when SUCCESS happens.*** The notes can be found at [compiler-optimizations/fpo.md](https://github.com/aggrawal-ankur/knowledge-base/blob/main/compiler-optimizations/pre/pre.md)

This feels like branch reduction, then PRE.

But GCC does it very selectively **where it sees profitability**. Let's start with `init`. It has 4 branches (.init_first, .invalid_sizes, .sizemax_overflow, .malloc_failed).
  - .init_first is hoisted.
  - .invalid_sizes isn't hoisted.
  - .sizemax_overflow is hoisted.
  - .malloc_failed isn't hoisted.

Why something is hoisted doesn't matter much when we ask **why something is not hoisted!**
  - Why .malloc_failed is not hoisted? Maybe the compiler reasoned that malloc will clobber `rax`. Makes sense.
  - Why .invalid_sizes isn't hoisted? I've no answer to it yet.

If we notice a little more, we'll find that gcc is using `ecx` for saving the return code and moving it into `eax` while returning. `eax` is never used directly, not even a single time. This raises an even bigger question, when the compiler was clever enough to find that the return code can be stored in ecx and moved to eax, why .invalid_sizes is not hoisted?

A question of profitability makes sense here. *What makes hoisting something profitable while something else expensive?* This is also when you are hoisting somewhere else and then copying into eax *which effectively does what kind of optimization is something amusing right now, at least until the reasoning isn't clear.*

---

Although I am done for today, with pretty unclear thoughts, and a tired mind and body, my learning for today is that ***in compiler generated assembly, everything exists for a reason, logical or economical***. Earlier I thought it is only logical, but now I understand it can be economical as well.

I don't remember when I felt tired both mentally and physically the last time. It was definitely before analyzing the IRs and writing dynarr.asm, in early to mid January 2026. Today I feel tired after a long time.

# Day 3

## Hoisting led Branch Reduction (continued)

Good morning. I woke up fresh and relaxed. Before even getting out of bed, I had the first insight of the day. ***Is clever register usage worth an optimization trick? Is giving high attention to how the intent can be expressed is worth of an optimization technique?***

I opened VS Code a few moments ago and the first thing I saw is "I am pushing `r13, r14, r15` early but when it comes to preserving rdx (which contains cap), I am using rcx, instead of r15, which is the right place for preserving rdx. Later I am moving rcx into r15.
```asm
# if (cap > SIZE_MAX/elem_size) return SIZEMAX_OVERFLOW;
  mov  rcx, rdx    # preserve rdx (cap) as mul will clobber rdx
  mov  rax, rdx    # rax = cap
  mul  rsi         # (rax * rsi) == (cap * elem_size)  Result => rdx:rax {rdx:64-127 bits, rax:0-63 bits}
  test rdx, rdx    # Check if result overflowed to rdx
  jnz  .sizemax_overflow     # Jump if rdx is non-zero

# malloc; preserve rdi, rsi and rdx
  mov r13, rdi
  mov r14, rsi
  mov r15, rcx     # Not rdx because it has been used in imul and preserved in rcx
```

This can be simplified to:
```asm
# if (cap > SIZE_MAX/elem_size) return SIZEMAX_OVERFLOW;
  mov  r15, rdx    # preserve rdx (cap) in r15 as mul will clobber rdx to store 64-127 bits of the result (overflow)
  mov  rax, rdx    # rax = cap
  mul  rsi         # (rax * rsi) == (cap * elem_size)  Result => rdx:rax {rdx:64-127 bits, rax:0-63 bits}
  test rdx, rdx    # Check if result overflowed to rdx
  jnz  .sizemax_overflow    # Jump if rdx is non-zero

# malloc; preserve rdi and rsi
  mov r13, rdi
  mov r14, rsi
```

In init(), I've hoisted .init_first(-1) and .invalid_sizes(-2) because they can be hoisted and I left -3 and -4 because they can't be unless I use a third register, like gcc at -O1, which doesn't make sense to me.

---

Before we hoist return values in other procedures, I want to note that gcc at -O1 does it very selectively. Like extend has only SUCCESS being hoisted. Plus, gcc is optimizing for how many instructions actually execute at runtime, instead of total number of instructions in the procedure.

It is very much possible that my act of hoisting return values only make the code unoptimized. One thing I recently thought is that avoiding error is something I try to do my best. And we do try to optimize the code for success paths. Right now, I am hoisting return codes for error paths, why don't I do this for success paths instead? Like in extend, I didn't hoist `(arr->count+add_bytes <= arr->capacity)` when this is a likely path in normal conditions, while error paths are unlikely by default unless you manipulate them.

So, I can take an approach where I can hoist return paths which are accessed frequently and eax is not used in between. This way, I hoist one value and enjoy it on more than one conditional jump. Second, I can hoist success paths if there are multiple of them, like extend has. Although I am sure this is not the case with most of the functions, but I had the idea so I shared it.

This framework is better than hoisting values in wild.

---

I've hoisted what I thought is worth hoisting and the number of lines have reduced to 805 from 858 (53 lines). I've added a bunch of new comments to account for this change at some places and modified register usage in init where I am using r15 directly instead of rcx to preserve rdx.

As I read gcc at -O1, seeing patterns that look weird and don't make complete sense, I've realized that ***not very choice is logical, some choices are economical. There is always a possibility to reverse the logical reasoning the compiler used, but reversing economical reasoning is something not possible until I understand the compiler's cost model.***

## Restructuring and removing redundant comments

This one is for myself. Whenever I do anything, resetting comments always slide in. So I did it once for all, so that I can reduce it to bare minimum.

805 lines -> 742 lines (63 lines)

## Inline and setidx boundcheck

Inlining boundcheck is a very simple because the computation it involves is very simple.

This is boundcheck:
```c
int boundcheck(size_t lb, size_t ub, size_t idx){ return (idx >= lb && idx < ub); }
```

boundcheck expects idx as a size_t value, that means, idx belongs to `[0, 2^64 - 1)`, so we don't have to check for (idx <= lb). We only have to check for (idx < ub). That's it.

Take getelement, for example:
```asm
mov rdx, rsi                  # Arg3 (rdx=idx)
mov rsi, QWORD PTR 16[rdi]    # Arg2 (rsi=arr->count)
xor rdi, rdi                  # Arg1 (rdi=0)
call boundcheck
test eax, eax
jz   .null_ret    # NULL
```
This assembly can be replaced with:
```asm
cmp rsi, 16[rdi]    # idx < ub
jae .ret_block_p6
```
Plus, we don't have to preserve rdi and rsi anymore.

I've inlined boundcheck at 4 places and all tests passed.

742 lines -> 694 lines (48 lines)

